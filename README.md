# StockPy

StockPy is a **data pipeline project** built to extract, process, and analyze stock market data from **B3 (Brazilian Stock Exchange)**, as well as related financial news.
It was developed as part of the **Tech Challenge - Advanced Machine Learning Phase**, focusing on **Big Data Architecture** and **Data Engineering practices** using **AWS services**.

## ğŸ“Œ Project Purpose

The main goal of StockPy is to build an automated **ETL (Extract, Transform, Load) pipeline** to track and analyze selected B3 stocks and their related news articles, providing refined datasets for analytics.

## ğŸš€ Features

### 1. Stock Data Pipeline

* Extracts stock data (daily granularity) from B3.
* Stores raw data in **Amazon S3** in **Parquet** format with daily partitioning.
* Executes **ETL transformations in AWS Glue**, including:

  * Aggregation and summarization (average closing price, total volume).
  * Renaming and standardizing columns.
  * Calculations based on dates (e.g., daily variation in closing prices).
* Loads transformed data back into **S3 (refined zone)** and catalogs it in **Glue Catalog**.
* Data can be queried with **Amazon Athena**.

### 2. News Data Pipeline

* Crawls financial news articles related to selected companies using **Google News**.
* Cleans and processes news data:

  * Standardized column naming.
  * Removal of duplicates and inconsistent records.
  * Date validation between publication and extraction.
* Stores processed news in **S3 (Parquet format)** for analytics.

### 3. AWS Orchestration

* **EventBridge** triggers events for data ingestion.
* **Step Functions** orchestrate the ETL pipeline execution.
* **Terraform** is used for infrastructure-as-code (IaC) deployment, managing S3, Glue, EventBridge, and Step Functions.

## ğŸ—ï¸ Architecture

The project follows a **modular AWS-based pipeline**:

1. **S3** â€“ Data lake storage (raw and refined data).
2. **Glue** â€“ ETL jobs for transformation and cataloging.
3. **EventBridge** â€“ Scheduling and event triggers.
4. **Athena** â€“ SQL queries over refined datasets.
5. **Step Functions** â€“ Workflow orchestration.

![Pipeline Architecture](docs/AWS_Data_Pipeline_Architecture.png)



## ğŸ“‚ Repository Structure

```
StockPy/
â”œâ”€â”€ docs/                         # Documentation and resources
â”‚   â”œâ”€â”€ AWS_Data_Pipeline_Architecture.png
â”‚   â””â”€â”€ Tech Challenge Fase 2 - Machine Learning AvanÃ§ado.pdf
â”œâ”€â”€ scripts/                      # Data extraction and transformation scripts
â”‚   â”œâ”€â”€ extract_news_job.py
â”‚   â”œâ”€â”€ extract_stocks_job.py
â”‚   â”œâ”€â”€ queries.sql
â”‚   â”œâ”€â”€ transform_news_job.py
â”‚   â””â”€â”€ transform_stocks_job.py
â”œâ”€â”€ terraform/                    # Infrastructure as Code (Terraform)
â”‚   â”œâ”€â”€ s3/                       # S3 configuration
â”‚   â”œâ”€â”€ glue/                     # Glue ETL jobs
â”‚   â”œâ”€â”€ eventbridge/              # EventBridge rules
â”‚   â””â”€â”€ stepfunctions/            # State Machine orchestration
â”œâ”€â”€ setup/                        # Setup and formatting utilities
â”‚   â””â”€â”€ black_formatter.sh
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ LICENSE                       # Project license
â””â”€â”€ README.md                     # Documentation
```



## ğŸ› ï¸ Tech Stack

* **Python** (ETL scripts, data transformations)
* **AWS S3** (data lake)
* **AWS Glue** (ETL and cataloging)
* **AWS Athena** (SQL queries on data lake)
* **AWS EventBridge** (event-driven architecture)
* **AWS Step Functions** (workflow orchestration)
* **Terraform** (Infrastructure as Code)



## ğŸ“Š Stocks and Sectors Covered

The project tracks B3 companies based on the **BEST investment criteria** (Barsi Method: Banks, Energy, Sanitation/Insurance, Telecommunications):

| Sector             | Ticker | Company         |
| ------------------ | ------ | --------------- |
| Bank               | ITUB4  | ItaÃº Unibanco   |
| Bank               | BBAS3  | Banco do Brasil |
| Energy             | ISAE4  | ISA Energia     |
| Energy             | CPFE3  | CPFL Energia    |
| Sanitation         | SBSP3  | Sabesp          |
| Sanitation         | SAPR4  | Sanepar         |
| Insurance          | PSSA3  | Porto Seguro    |
| Insurance          | BBSE3  | BB Seguridade   |
| Telecommunications | VIVT3  | Vivo            |
| Telecommunications | INTB3  | Intelbras       |



## âš™ï¸ Installation & Setup

1. Clone the repository:

   ```bash
   git clone https://github.com/LucasTechAI/StockPy.git
   cd StockPy
   ```

2. Create a virtual environment and install dependencies:

   ```bash
   python -m venv venv
   source venv/bin/activate   # Mac/Linux
   venv\Scripts\activate      # Windows
   pip install -r requirements.txt
   ```

3. Deploy AWS infrastructure with Terraform:

   ```bash
   cd terraform
   terraform init
   terraform apply
   ```

4. Run ETL jobs locally (optional):

   ```bash
   python scripts/extract_stocks_job.py
   python scripts/transform_stocks_job.py
   ```



## ğŸ“– Documentation

* [Project Challenge Requirements](docs/Tech%20Challenge%20Fase%202%20-%20Machine%20Learning%20AvanÃ§ado.pdf)
* [Pipeline Architecture Diagram](docs/AWS_Data_Pipeline_Architecture.png)



## ğŸ‘¤ Author

**Lucas Mendes Barbosa**: Data Scientist (NLP, LLMs, OCR)
* ğŸ“§ Email: [lucas.mendestech@gmail.com](mailto:lucas.mendestech@gmail.com)
* ğŸ”— [LinkedIn](https://www.linkedin.com/in/lucas-mendes-barbosa/)
* ğŸ’» [GitHub](https://github.com/LucasTechAI/StockPy)
* ğŸŒ [Portfolio](https://musicmoodai.com.br/)



## ğŸ“œ License

This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.

